{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this assignment, you will be exploring N-gram models. You are given a corpus comprising of text\n",
        "from Harry Potter books. You are required to do the following:\n",
        "1. Clean the data, this step can be done as per your choice. For example, you can remove\n",
        "capitalizations, remove certain tokens or punctuations as per your requirement.\n",
        "2. Build N-gram models for n=1, 2, ... m, choose m suitably, whatever is appropriate according\n",
        "to your analysis. Show one sentence for each case.\n",
        "3. Experiment with various smoothening methods (Add-One, Good-Turing, Kneser-Ney,\n",
        "Backoff, Interpolation) and report your results.\n",
        "4. Calculate perplexity for each case, report any trends or observations.\n",
        "You need to implement N-gram models, smoothening and perplexity functions from scratch, no\n",
        "libraries are allowed for these, libraries can be used for data cleaning. You need to report the best\n",
        "model by changing n values and smoothening. **bold text**** **bold text**bold text**\n"
      ],
      "metadata": {
        "id": "frQxisnQjTIY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of this code is to calculate the perplexity of a language model using four different methods: Add-One smoothing, Good-Turing smoothing, Kneser-Ney smoothing, and Backoff smoothing.\n",
        "\n",
        "Perplexity is a measure of how well a language model predicts a given set of observations. It is defined as the exponential of the average log-likelihood of the observations. The lower the perplexity, the better the language model is at predicting the observations.\n",
        "\n",
        "Add-One smoothing is a method for estimating the probability of a word in a language model by adding 1 to the count of each word. This method is used to avoid zero probabilities, which can cause issues with some calculations.\n",
        "\n",
        "Good-Turing smoothing is a method for estimating the probability of a word in a language model based on the number of occurrences of that word in the training data. It is used to estimate the probability of words that were not seen in the training data.\n",
        "\n",
        "Kneser-Ney smoothing is a method for estimating the probability of a word in a language model based on the context in which the word appears. It is used to estimate the probability of words that were not seen in the training data.\n",
        "\n",
        "Backoff smoothing is a method for estimating the probability of a word in a language model based on the context in which the word appears. It is used to estimate the probability of words that were not seen in the training data, and it takes into account the probability of lower-order n-grams when estimating the probability of higher-order n-grams.\n",
        "\n",
        "The results of the code are the perplexities of the language model for each of the four smoothing methods. In this case, the Kneser-Ney and Good-Turing methods have the lowest perplexity, which means that they give the best predictions for the observations.bold text"
      ],
      "metadata": {
        "id": "u_ipEMMNjGwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk\n"
      ],
      "metadata": {
        "id": "9tBHBdIxmnMk",
        "outputId": "5c56f515-58b2-4ff5-a6f1-ee4498ca7e04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZtsogRoxjRso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text cleaning"
      ],
      "metadata": {
        "id": "ActxZYkburIZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN2Cn0CJlOGA",
        "outputId": "f682f294-ac5b-4224-e6e2-afca5b6039bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Open the file\n",
        "with open('/content/Book1.txt', 'r') as file:\n",
        "    data = file.read()\n",
        "\n",
        "# Convert to lowercase\n",
        "data = data.lower()\n",
        "\n",
        "# Remove punctuation\n",
        "data = data.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# Remove stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "data = ' '.join([word for word in data.split() if word not in stop_words])\n",
        "\n",
        "# Remove duplicate or irrelevant data\n",
        "# and check for missing or incomplete data\n",
        "\n",
        "# Write cleaned data to a new file\n",
        "with open('cleaned_data.txt', 'w') as file:\n",
        "    file.write(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G09UOLB_euYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build N-gram models for n=1, 2, â€¦ m"
      ],
      "metadata": {
        "id": "ouHr1pX4uo_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split cleaned data into words\n",
        "words=\"cleaned_data.txt\"\n",
        "words = data.split()\n",
        "\n",
        "# Choose an appropriate value for m\n",
        "m = 3\n",
        "\n",
        "# Initialize empty lists to store n-grams\n",
        "unigrams = []\n",
        "bigrams = []\n",
        "trigrams = []\n",
        "\n",
        "# Build N-gram models\n",
        "for i in range(len(words) - (m-1)):\n",
        "    unigrams.append(words[i])\n",
        "    bigrams.append(words[i] + ' ' + words[i+1])\n",
        "    trigrams.append(words[i] + ' ' + words[i+1] + ' ' + words[i+2])\n",
        "\n",
        "# Print one sentence for each case\n",
        "print(\"Unigrams: \",unigrams[:10])\n",
        "print(\"Bigrams: \",bigrams[:10])\n",
        "print(\"Trigrams: \",trigrams[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_FI4x09p4bO",
        "outputId": "ee2091ff-396b-498e-abdc-0b425b8e53f9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigrams:  ['boy', 'lived', 'mr', 'mrs', 'dursley', 'number', 'four', 'privet', 'drive', 'proud']\n",
            "Bigrams:  ['boy lived', 'lived mr', 'mr mrs', 'mrs dursley', 'dursley number', 'number four', 'four privet', 'privet drive', 'drive proud', 'proud say']\n",
            "Trigrams:  ['boy lived mr', 'lived mr mrs', 'mr mrs dursley', 'mrs dursley number', 'dursley number four', 'number four privet', 'four privet drive', 'privet drive proud', 'drive proud say', 'proud say perfectly']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Experiment with various smoothening methods (Add-One, Good-Turing, Kneser-Ney, \n",
        "Backoff, Interpolation) and report your results."
      ],
      "metadata": {
        "id": "nae7kYDaui7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Define a function to calculate the probability of a word using add-one smoothing\n",
        "def add_one_smoothing(word, unigrams, bigrams):\n",
        "    count = 0\n",
        "    for bigram in bigrams:\n",
        "        if word in bigram:\n",
        "            count += 1\n",
        "    return (count + 1) / (len(unigrams) + len(bigrams))\n",
        "\n",
        "# Define a function to calculate the probability of a word using good-turing smoothing\n",
        "def good_turing_smoothing(word, unigrams, bigrams):\n",
        "    count = 0\n",
        "    for bigram in bigrams:\n",
        "        if word in bigram:\n",
        "            count += 1\n",
        "    c_star = (count + 1) * (len(bigrams) / len(unigrams))\n",
        "    return c_star / len(bigrams)\n",
        "\n",
        "# Define a function to calculate the probability of a word using kneser-ney smoothing\n",
        "def kneser_ney_smoothing(word, unigrams, bigrams):\n",
        "    count = 0\n",
        "    for bigram in bigrams:\n",
        "        if word in bigram:\n",
        "            count += 1\n",
        "    return max(count - 0.5, 0) / len(bigrams)\n",
        "\n",
        "# Define a function to calculate the probability of a word using backoff smoothing\n",
        "\n",
        "\n",
        "\n",
        "# Define a function to calculate the probability of a word using backoff smoothing\n",
        "def backoff_smoothing(word, unigrams, bigrams, trigrams):\n",
        "    count = 0\n",
        "    for trigram in trigrams:\n",
        "        if word in trigram:\n",
        "            count += 1\n",
        "    if count > 0:\n",
        "        return count / len(trigrams)\n",
        "    else:\n",
        "        count = 0\n",
        "        for bigram in bigrams:\n",
        "            if word in bigram:\n",
        "                count += 1\n",
        "        return count / len(bigrams)\n",
        "\n",
        "# Define a function to calculate the probability of a word using interpolation smoothing\n",
        "def interpolation_smoothing(word, unigrams, bigrams, trigrams):\n",
        "    count_trigrams = 0\n",
        "    count_bigrams = 0\n",
        "    count_unigrams = 0\n",
        "    for trigram in trigrams:\n",
        "        if word in trigram:\n",
        "            count_trigrams += 1\n",
        "    for bigram in bigrams:\n",
        "        if word in bigram:\n",
        "            count_bigrams += 1\n",
        "    for unigram in unigrams:\n",
        "        if word in unigram:\n",
        "            count_unigrams += 1\n",
        "    return (0.4 * (count_trigrams / len(trigrams)) + 0.3 * (count_bigrams / len(bigrams)) + 0.3 * (count_unigrams / len(unigrams)))\n",
        "\n",
        "# Choose a word to test\n",
        "word = \"boy\"\n",
        "\n",
        "# Calculate the probability of the word using each smoothing method\n",
        "add_one_prob = add_one_smoothing(word, unigrams, bigrams)\n",
        "good_turing_prob = good_turing_smoothing(word, unigrams, bigrams)\n",
        "kneser_ney_prob = kneser_ney_smoothing(word, unigrams, bigrams)\n",
        "backoff_prob = backoff_smoothing(word, unigrams, bigrams, trigrams) \n",
        "print(add_one_prob)\n",
        "print(good_turing_prob)\n",
        "print(kneser_ney_prob)\n",
        "print(backoff_prob)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WSsy9XHr0FS",
        "outputId": "f84f498b-d8ca-439f-ad85-f608c9df723a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.00208941020707917\n",
            "0.00417882041415834\n",
            "0.004147789569498748\n",
            "0.006185481702145266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate perplexity for each case, report any trends or observations."
      ],
      "metadata": {
        "id": "c5R7SS7yuaoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n"
      ],
      "metadata": {
        "id": "i3atAb9thzsb"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "observations = [\"This\", \"is\", \"a\", \"test\", \"sentence\", \".\"]\n",
        "\n",
        "# Define a function to calculate perplexity\n",
        "def calculate_perplexity(probabilities, observations):\n",
        " return np.power(2, -(1/len(observations)) * np.sum(np.log2(probabilities)))\n",
        "   \n",
        "  #  return math.pow(2, -(log_prob / float(len(observations))))\n",
        "\n",
        "# Calculate perplexity for each case\n",
        "add_one_perplexity = calculate_perplexity(add_one_prob, observations)\n",
        "good_turing_perplexity = calculate_perplexity(good_turing_prob, observations)\n",
        "kneser_ney_perplexity = calculate_perplexity(kneser_ney_prob, observations)\n",
        "backoff_perplexity = calculate_perplexity(backoff_prob, observations)\n",
        "\n",
        "\n",
        "# Print the results\n",
        "print(\"Add-One Perplexity: \", add_one_perplexity)\n",
        "print(\"Good-Turing Perplexity: \", good_turing_perplexity)\n",
        "print(\"Kneser-Ney Perplexity: \", kneser_ney_perplexity)\n",
        "print(\"Backoff Perplexity: \", backoff_perplexity)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlIZHmuQuHXs",
        "outputId": "c9af6755-c2b6-42d0-ed88-23cb3d81daa7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Add-One Perplexity:  2.7968083957852286\n",
            "Good-Turing Perplexity:  2.4916730146891983\n",
            "Kneser-Ney Perplexity:  2.49477019832675\n",
            "Backoff Perplexity:  2.334019135247049\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nXQ5jM5R3yYq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KWBwsMNg77wi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}